{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.3_Linear_Algebra.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOviOD6mbsZYLZLjF099/yY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagarladla/D2Ls_Deep_Learning-Pytorch/blob/master/2_3_Linear_Algebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGJeeVpr990Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3F3kaWM_Wa1",
        "colab_type": "code",
        "outputId": "92c4514e-ea82-4d5c-97eb-0369c8194d39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Scalars\n",
        "x=torch.tensor([3.])\n",
        "y=torch.tensor([2.])\n",
        "\n",
        "x+y,x*y,x-y,x/y,x**y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([5.]), tensor([6.]), tensor([1.]), tensor([1.5000]), tensor([9.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0i0ah7xAAfq",
        "colab_type": "code",
        "outputId": "bbc97128-4906-49ed-c1a5-63b61f36745a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Vectors\n",
        "x=torch.arange(4)\n",
        "x[3]\n",
        "\n",
        "len(x) # `Length of vector` is `dimension of vector`"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqw-HZQLAkGD",
        "colab_type": "code",
        "outputId": "9bd4daab-aeea-4ad5-bd5d-92066b8e181c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "A = torch.arange(20).reshape(5,-1)\n",
        "A.T\n",
        "\n",
        "# Symmetric matrix: B = B.T\n",
        "B = torch.tensor([[1,2,3],[2,0,4],[3,4,5]])\n",
        "print(B)\n",
        "print(B.T)\n",
        "\n",
        "B==B.T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [2, 0, 4],\n",
            "        [3, 4, 5]])\n",
            "tensor([[1, 2, 3],\n",
            "        [2, 0, 4],\n",
            "        [3, 4, 5]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True],\n",
              "        [True, True, True],\n",
              "        [True, True, True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoJGBlwECuqE",
        "colab_type": "code",
        "outputId": "0cdd2af1-eebe-4131-be67-1b5293b7401b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X=torch.arange(24).reshape(2,3,4)\n",
        "X\n",
        "\n",
        "A=torch.arange(20, dtype=torch.float32).reshape(5,4)\n",
        "B=A.clone() # Copy of A to B by allocating new memory\n",
        "A,A+B\n",
        "A*B # Hadamard product, elementwise multiplication of matrices\n",
        "\n",
        "a=2\n",
        "X=torch.arange(24).reshape(2,3,4)\n",
        "X\n",
        "a+X\n",
        "(a*X).shape"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dq3GFZ2LqJy",
        "colab_type": "code",
        "outputId": "ed6b8b1e-7b55-4bbb-c5d2-c7d8323d7d39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Reducing sum ie dissolving of dimensions\n",
        "x=torch.arange(4, dtype=torch.float32)\n",
        "x,x.sum()\n",
        "\n",
        "A.shape, A.sum()\n",
        "\n",
        "A_sum_axis0=A.sum(axis=0)\n",
        "print(\"Along axis 0: \", A_sum_axis0, A_sum_axis0.shape)\n",
        "\n",
        "A_sum_axis1=A.sum(axis=1)\n",
        "print(\"Along axis 1: \",A_sum_axis1, A_sum_axis1.shape)\n",
        "\n",
        "A.sum(axis=[0,1]) # Same as A.sum()\n",
        "\n",
        "# Mean or average\n",
        "A.mean(), A.sum()/A.numel()\n",
        "\n",
        "A.mean(axis=0), A.sum(axis=0)/A.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Along axis 0:  tensor([40., 45., 50., 55.]) torch.Size([4])\n",
            "Along axis 1:  tensor([ 6., 22., 38., 54., 70.]) torch.Size([5])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvONaXt_OE_r",
        "colab_type": "code",
        "outputId": "5ba8a04e-93e2-446c-dbc0-6d694b84d1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Non reducing sum ie keepdims=True\n",
        "\n",
        "sum_A=A.sum(axis=1, keepdims=True)\n",
        "sum_A\n",
        "\n",
        "A/sum_A\n",
        "\n",
        "A.cumsum(axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  6.,  8., 10.],\n",
              "        [12., 15., 18., 21.],\n",
              "        [24., 28., 32., 36.],\n",
              "        [40., 45., 50., 55.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtriUL9xPbZ8",
        "colab_type": "code",
        "outputId": "9dea0f3e-a610-407e-aa2a-8ba31facc622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# dot product\n",
        "y=torch.ones(4, dtype=torch.float32)\n",
        "x, y\n",
        "\n",
        "torch.dot(x,y), torch.sum(x*y) # elementwise multiplication & sum = dot product"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(6.), tensor(6.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_ilepsMR34q",
        "colab_type": "text"
      },
      "source": [
        "- given data values in vector denoting <math xmlns=\"http://www.w3.org/1998/Math MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"bold\"><b>x </mi></mrow><mo>&#x2208;<!-- ∈ --></mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\"> R</mi></mrow><sup>d</sup></msup></b></math> and set of weights <math xmlns=\"http://www.w3.org/1998/Math MathML\"><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"bold\"><b>w </mi></mrow><mo>&#x2208;<!-- ∈ --></mo><msup><mrow class=\"MJX-TeXAtom-ORD\"><mi mathvariant=\"double-struck\"> R</mi></mrow><sup>d</sup></msup></b></math>, then weighted sum of values in <b>x</b> w.r.t. <b>w</b> expressed as <b>x<sup>T</sup>w</b>. \n",
        "\n",
        "- When weights are `non -ve` & `w.sum()=1`, <em>\"dot product expresses weighted average\"</em>.\n",
        "\n",
        "- After normalizing <b>x</b> & <b>w</b> both vectors to unit length, <em>\"dot product expresses cosine of angle between them\".</em>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsrdvLVuUgN9",
        "colab_type": "code",
        "outputId": "192b7404-b657-42c2-cbdd-b780f646b589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "A.shape, x.shape, torch.mv(A,x)\n",
        "\n",
        "A, x\n",
        "\n",
        "B=torch.ones(4,3)\n",
        "torch.mm(A,B)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.,  6.,  6.],\n",
              "        [22., 22., 22.],\n",
              "        [38., 38., 38.],\n",
              "        [54., 54., 54.],\n",
              "        [70., 70., 70.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PZasKypCusF",
        "colab_type": "code",
        "outputId": "b828094e-917a-43fc-8c4e-95892c622464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# l2 norm\n",
        "u=torch.tensor([3.0, -4.0])\n",
        "torch.norm(u)\n",
        "\n",
        "# l1 norm\n",
        "torch.abs(u).sum()\n",
        "\n",
        "# Frobenius norm\n",
        "X = torch.ones(4,9)\n",
        "torch.norm(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAPujNJVFHs-",
        "colab_type": "text"
      },
      "source": [
        "# Norm properties\n",
        "![Norm Properties](https://raw.githubusercontent.com/sagarladla/D2Ls_Deep_Learning-Pytorch/master/Norms/1_1.jpg)\n",
        "\n",
        "# l<sub>2</sub> norms\n",
        "![l2 norms](https://raw.githubusercontent.com/sagarladla/D2Ls_Deep_Learning-Pytorch/master/Norms/1_2.jpg)\n",
        "\n",
        "# l<sub>1</sub> norms\n",
        "![l1 norms](https://raw.githubusercontent.com/sagarladla/D2Ls_Deep_Learning-Pytorch/master/Norms/1_3.jpg)\n",
        "\n",
        "# general/l<sub>p</sub> norms and Ferobenius norms\n",
        "![general or lp norms & Frobenius norms](https://raw.githubusercontent.com/sagarladla/D2Ls_Deep_Learning-Pytorch/master/Norms/1_4.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok6ITrs5JYXw",
        "colab_type": "text"
      },
      "source": [
        "In DL we do optimization problems: \n",
        "- <b>`maximize`</b> probability assigned to observed data,\n",
        "- <b>`minimize`</b> distance between prediction & ground truth.\n",
        "\n",
        "### Assigning vector representations to - words, articles, products, such that :\n",
        "\n",
        "- `distance maximizes` among dissimilar items\n",
        "- `distance minimizes` among similar items.\n",
        "\n",
        "# Thus, more often objectives (components of DL algos) are expressed as *__norms__*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KVOyy4ZLzer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exercises\n",
        "\n",
        "# 1. (A.T).T = A // high school work\n",
        "# A = torch.arange(12).reshape(4,-1)\n",
        "# (A.T).T == A\n",
        "\n",
        "# 2. A.T + B.T = (A+B).T\n",
        "# B = torch.ones(4, 3)\n",
        "# A.T + B.T == (A+B).T\n",
        "\n",
        "# 3. Bcuz A = A.T. That's why A + A.T = always symmetric.\n",
        "\n",
        "# 4. len(torch.tensor(dim=(2,3,4)))\n",
        "# X = torch.arange(24).reshape(2,3,4)\n",
        "# len(X)\n",
        "\n",
        "# 5. Yes, len(X) always corresponds to certain axis of X. That axis is always outermost most axis, ie axis=0\n",
        "\n",
        "# 6. Reason - Bcuz it is not a square matrix, ie rows != cols. A/A.sum(axis=1)\n",
        "# 7. \n",
        "# 8. tensor.shape = (2,3,4)\n",
        "# tensor containing two matrices of 3 x 4 each\n",
        "# sum(axis=0), dimension = 3 x 4\n",
        "# sum(axis=1) (rows), dimension = 2 x 4 x 1\n",
        "# sum(axis=2) (cols), dimension = 2 x 3 x 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}